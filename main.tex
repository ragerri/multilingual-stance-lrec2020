\documentclass[10pt, a4paper]{article}
\usepackage{lrec}
%\usepackage{multibib}
%\newcites{languageresource}{Language Resources}
\usepackage{graphicx}
\usepackage{tabularx}
\usepackage{soul}
% for eps graphics
\usepackage{epstopdf}
\usepackage[latin1]{inputenc}
\usepackage{hyperref}
\usepackage{xstring}
\usepackage{todonotes}
\newcommand{\secref}[1]{\StrSubstitute{\getrefnumber{#1}}{.}{ }}

\title{Multilingual Stance Detection: The Catalonia Independence Corpus}

\name{Elena Zotova$^1$, Rodrigo Agerri$^1$, Manuel Nu\~nez$^2$, German Rigau$^1$}

\address{$^1$ IXA Group, HiTZ Centre, University of the Basque Country UPV/EHU, Donostia-San Sebastian, Spain\\
$^2$ Intercom Strategys, Madrid, Spain\\
zotova.el@gmail.com, rodrigo.agerri@ehu.eus, manuel.nunez@lunigtuk.com,
german.rigau@ehu.eus}


\abstract{Stance detection aims to determine the attitude of a given text with respect to a specific topic or claim. While stance detection has been fairly well researched in the last years, most the work has been focused on English. This is mainly due to the relative lack of annotated data in other languages. The TW-10 Referendum Dataset released at IberEval 2018 is a previous effort to provide multilingual stance-annotated data in Catalan and Spanish. Unfortunately, the TW-10 Catalan subset is extremely imbalanced. This paper addresses these issues by presenting a new multilingual dataset for stance detection in Twitter for the Catalan and Spanish languages, with the aim of facilitating research on stance detection in multilingual and cross-lingual settings. The dataset is annotated with stance towards one topic, namely, the independence of Catalonia. We also provide a semi-automatic method to annotate the dataset based on a categorization of Twitter users. We experiment on the new corpus with a number of supervised approaches, including linear classifiers and deep learning methods. Comparison of our new corpus with the with the TW-1O dataset shows both the benefits and potential of a well balanced corpus for multilingual and cross-lingual research on stance detection. Finally, we establish new state-of-the-art results on the TW-10 dataset, both for Catalan and Spanish.\\ \newline \Keywords{Stance Detection, Text Categorization, Less-Resourced Languages} }

\begin{document}

\maketitleabstract

\section{Introduction}

The rise of social media has given rise to the ``fake news'' phenomenon. According to the Fake News Challenge, ``Fake news, defined by the New York Times as ``a made-up story with an intention to deceive''\footnote{\url{https://www.nytimes.com/2016/12/06/us/fake-news-partisan-republican-democrat.html}}, often for a secondary gain, is arguably one of the most serious challenges facing the news industry today.''\footnote{\url{http://www.fakenewschallenge.org/}}

Determining the veracity of a given document or story, namely, whether it is fake or legitimate, is a very complex task, even for expert fact-checkers. Thus, previous work breaks down the fake news detection task in different stages, the first of which is establishing what other news sources are saying about the given document or story (whether they agree, disagree, etc. with the news story), namely, determining their stance with respect to that document or news story. Following this, the first stage of the Fake News Challenge was \emph{Stance Detection}. This decision was supported by two main ideas: (i) a stance detection system should allow a human fact checker to enter a document (headline, message, claim, etc.) and retrieve the top documents from other news sources that agree, disagree or discuss the given document and, (ii) based on the previous step, it would be possible to build a ``truth-labeling'' system based on the weighted credibility of the various news organizations from which the stance has been retrieved.

Automatic stance detection has been defined as the task of classifying the attitude expressed in a text towards a given target or claim. Most of the work on stance detection has been undertaken in English using the data provided by the Detecting Stance in Tweets shared task organized at SemEval 2016 \cite{mohammad-etal-2016-semeval}, RumourEval 2017 \cite{derczynski-etal-2017-semeval} and the Fake News Challenge. The SemEval 2016 task was formulated as follows: given a tweet text and a target entity or topic, automatic natural language systems must determine whether the tweet expresses a stance in \textbf{favor} of the given target, \textbf{against} the given target, or whether \textbf{none} of those inferences are likely. For example, consider the following target$-$tweet pairs:

\begin{quote}
\textbf{Tweet:} \textit{I still remember the days when I prayed God for strength.. then suddenly God gave me difficulties to make me strong. Thank you God! \#SemST}

\textbf{Target:} Atheism

\textbf{Stance:} AGAINST

\vspace{0.5cm}

\textbf{Tweet:} \textit{@PH4NT4M @MarcusChoOo @CheyenneWYN women. The term is women. Misogynist! \#SemST}

\textbf{Target:} Feminist Movement

\textbf{Stance:} FAVOR
\end{quote}

These examples illustrate the nature of the task, in which tweets are very short, full of specific vocabulary, non-standard spelling grammar, emojis, hashtags, and high on irony and sarcasm. The task aimed to detect stance from single tweets, without taking into account the conversational structure of tweet threads or any information about authors.

Following the model of the SemEval 2016 initiative, two shared tasks were organized as part of IberEval workshop \cite{taule17,taule18}. They provided tweets annotated for Stance in Catalan and Spanish. The target of the 2017 edition was the ``Catalan Independence'' whereas the 2018 edition (TW-10 dataset) focused on the ``Catalan referendum on the 1st of october''. In both editions the classes distribution was hugely skewed, which makes it difficult to explore and compare stance detection methods in multilingual and cross-lingual settings.

In this context, we propose the new Catalan Independence Corpus (CIC) for stance detection in Catalan and Spanish. By doing so, we aim to promote research in other languages different to English. Furthermore, the corpus presents a balanced distribution between classes so that researchers can explore multilingual and cross-lingual methods.

The contributions of this paper are the following: (i) we present a new dataset in Catalan and Spanish to work on multilingual and cross-lingual stance detection; (ii) we propose a semi-automatic method to collect and annotate a corpus of tweets based on a categorization of Twitter users. This method partially alleviates the huge effort of manually annotating the corpus tweet by tweet; (iii) we report new state-of-the-art results on the TW-10 dataset of IberEval 2018 \cite{taule18}; (iv) comparison between results using our new corpus and the TW-10 dataset shows the benefits of providing a balanced multilingual corpus, and (v) both the datasets and code are made public to facilitate future research and reproducibility of results\footnote{\url{https://github.com/ixa-ehu/catalonia-independence-dataset}}.

\section{Related Work}\label{sec_related_work}

% First of all, stance detection can be viewed as a subtask of Opinion Mining being also closely related to Sentiment Analysis \cite{pang2008opinion} and Text Classification \cite{aggarwal2012survey}. It is a relatively new research area in NLP. From the beginning, the main approach for stance classification has been supervised machine learning. Initial works mainly focused on congressional debates \cite{Thomas06a} or debates in online forums \cite{somasundaran-wiebe-2009-recognizing,murakami-raymond-2010-support,Anand:2011:CRD:2107653.2107654,walker-etal-2012-stance,hasan-ng-2014-taking,sridhar-etal-2014-collective}. These domains are specific because the gold labels can easily be obtained. For instance, in \cite{sridhar-etal-2014-collective} approach the authors collect posts from various authors from social media debate sites and forums, where all posts are connected as dialogues or responses to the main post of the discussion. The posts are linked to one another by agreement or rebuttal links and are already "labelled" for stance, either {\emph PRO} or {\emph CONTRA}. Main approaches for debate stance detection have used sentiment and argument lexicons, statistical measures and counts, n-grams, repeated punctuation, part-of-speech tagging, syntactic dependencies and may others \cite{Wang8672602}.

% Research on Twitter posts started in 2014, and presented a new challenge to the NLP community since tweets are short, informal, full of misspellings, shortenings, slang and emoticons, and are produced in large amount with great velocity. Researchers \cite{DBLP:journals/corr/RajadesinganL14} were the first who determined stance at user level. They assumed that if many users retweet a particular pair of tweets in a short time, then this is likely that this pair of tweets had something in common and share the same opinion on the topic.

% The first Stance Detection in Tweets task\footnote{http://alt.qcri.org/semeval2016/task6/} was presented in 2016 as a part of SemEval challenge organized by the National Research Council Canada. The task aimed to detect stance from single tweets, without taking into account conversational structure of online debates and information about authors. The SemEval 2016 competition included two subtasks. Task A was formulated as follows: "given a tweet text and a target entity (person, organization, movement, policy, etc.), automatic natural language systems must determine whether the tweeter is in favor of the given target, against the given target, or whether neither inference is likely"  \cite{mohammad-etal-2016-semeval}. In Task B the goal was to detect stance in relation of unseen target. The baseline system designed for the challenge built by the organizers obtained the best results. The features used in the system were the following: word and character n-grams, average word embeddings, and sentiment features. A supervised system was trained using a support vector machine classifier \cite{mohammad-etal-2016-semeval}. Other approaches were based on convolutional neural networks (CNN) \cite{wei-etal-2016-pkudblab,VijayaraghavanS16}, recurrent neural network (RNN) \cite{DBLP:journals/corr/ZarrellaM16}, ensemble model \cite{Liu16}, maximum entropy classifier and domain dictionaries \cite{krejzl-steinberger-2016-uwb}, etc. Most of the participants used standard text classification features such as word and sentence vector embeddings and n-grams.

The state of the art is divided into two main approaches. First, those that rely on \emph{traditional} machine learning models combined with hand-engineered features \cite{Mohammad:2017:SST:3106680.3003433} or vector-based word representations (word embeddings) \cite{bohler-etal-2016-idi}. In particular, \cite{Mohammad:2017:SST:3106680.3003433} obtained the best results for the supervised setting of the SemEval 2016 task using a SVM classifier to learn word n-grams (1-, 2-, and 3-gram) and character n-grams (2-, 3-, 4-, and 5-gram) features, outperforming deep learning approaches \cite{zarrella-marsh-2016-mitre,wei-etal-2016-pkudblab}.

Among the deep learning systems published, the pkudlab system \cite{wei-etal-2016-pkudblab} proposed a Convolutional Neural Network (CNN) architecture combined with a voting scheme to guide the predictions instead of generating them based on the accuracy obtained in the validation set. The MITRE team \cite{zarrella-marsh-2016-mitre} employed two recurrent RNN classifiers: the first was trained to predict task-relevant hashtags on a large unlabeled Twitter corpus which was then used to initialize a second RNN to be trained on the SemEval 2016 training set. \cite{du2017stance} proposed a neural network-based model to incorporate target-specific information by means of an attention mechanism. Finally, \cite{sun-etal-2018-stance} proposed a hierarchical attention network to weigh the importance of various linguistic information, and learn the mutual attention between the document and the linguistic information.

It should be said that neural network approaches have been more successful so far for the SemeEval 2016 Task B (weakly-supervised setting). Apart from the previously mentioned systems \cite{wei-etal-2016-pkudblab}, \cite{augenstein-etal-2016-stance} proposed a bidirectional Long-Short Term Memory (LSTM) encoding model. First, the target is encoded by a LSTM network and then a second LSTM is used to encode the tweet using the encoding of the target as its initial state.

Another interesting work is that of \cite{DBLP:journals/corr/RajadesinganL14} who tried to determine stance at user level. Their assumption was that if many users retweeted a particular pair of tweets in a short time, then it is likely that this pair of tweets had something in common and share the same opinion on the topic.

As far as we know, most approaches to stance detection are developed for English, with the few exceptions that use the Catalan and Spanish data from IberEval 2017 and 2018 \cite{taule17,taule18} or the work of \cite{mohtarami-etal-2019-contrastive} using the Arabic corpus provided by \cite{baly-etal-2018-integrating}.

With respect to the ``MultiModal Stance Detection in tweets on Catalan \#1Oct Referendum'' task at IberEval 2018 (MultiStanceCat), the best results for Spanish were obtained by the uc3m team \cite{Segura-Bedmar18}. They presented a system based on bag-of-words with TF-IDF vectorization. They evaluated several of the most commonly used classifiers, obtaining a final 28.02 F1 macro score in the Spanish test data. The best result in Catalan subset was obtained by the {CriCa} team \cite{Cuquerella2018CriCaTM}. Their approach consisted of combining the Spanish and Catalan subsets to create a larger and more balanced corpus. They experimented with stemming of various lengths (three, four and five characters) and removing character suffixes from the word. Since Spanish and Catalan share many words, stemming helped to generalize. Additionally, it is quite common to encounter tweets containing words and expressions in both languages. Their final F1 macro was 30.68.

\section{Experimental Setup}\label{sec:experiment}

The development of the Catalonia Independence Corpus was motivated by the experiments performed on the IberEval TW-10 data. The result of those experiments showed that, due to the highly imbalanced nature of the TW-10 corpus, any comparison of systems across languages were not particularly meaningful. In this section we will summarize the setup for the experiments performed on both datasets, TW-10 and our new Catalonia Independence Corpus.

Apart from the data pre-processing described in Section \ref{sec:data-pre-processing}, we experimented with four different system architectures: (i) TF-IDF vectorization with a SVM classifier; (ii) a SVM trained with FastText word embeddings \cite{Grave18} for the representation of tweets; (iii) the FastText text classification system \cite{joulin-etal-2017-bag} with FastText word embeddings and, finally (iv) the Flair system \cite{akbik-etal-2018-contextual}, which implements a Recurrent Neural Network (RNN) for text classification that can be combined with static and context-based string embeddings. In the following, we describe the pre-processing and each of the architectures tested in both the TW-10 and the Catalonia Independence Corpus (CIC).

\subsection{Data Pre-processing}\label{sec:data-pre-processing}

Since each tweet in the TW-1O dataset is given in context, with the previous and the next tweet, we use them to obtain longer and richer texts for classification.

\textbf{Normalization}: We believe that normalization helps to reduce the number of features for TF-IDF feature representation and to maximize the number of words that correspond with the vocabulary of pre-trained word vector models. First, we remove all punctuation and any expression starting with "@", "RT", URLs and numbers. The next step is lowercasing and normalization of spelling: we remove repeated characters with one and replacing common shortened words to their normal form. For example, \textit{holaaaaaaa} is converted to \textit{hola}. However, we leave untouched consonants composed of two characters (\emph{tt, ll, rr}). Finally, diacritics are systematically removed.

\textbf{Lemmatization}: Next, we apply a simplified version of lemmatization consisting of replacing the word form with its lemma via dictionary look-up \footnote{\url{https://github.com/michmech/lemmatization-lists}}. If a word is not found we leave it in its original form. This method is not capable to resolve ambiguities. For example, the Spanish preposition \textit{para} (``for'') and the verb \textit{para} (``stop'') will be mapped to the same lemma, namely, \textit{parar} (the infinitive ``to stop'' in Spanish). Furthermore, named entities are sometimes wrongly lemmatized. To reduce the error rate, we manually edited the list of lemmas, and deleted the less frequent ambiguous words. In any case, our experiments showed that this type of lemmatization reduces dramatically the number of features helping to improve results for every experimental setting. In addition, it allows to deal with unseen words. For example, if the Spanish word \textit{andando} (walking) does not appear in the training corpus but another form of its lemma does, then both words will be recognized as having the same lemma, namely, the Spanish verb \textit{andar} (to walk).

\textbf{Tokenization}: We perform whitespace tokenization, also removing stopwords (auxiliary verbs, prepositions, articles, pronouns and the most frequent words) and words shorter than three characters.

\subsection{SVM+TF-IDF}\label{sec:tf-idf+svm}

\textbf{TF-IDF} (Term Frequency times Inverse Document Frequency) \cite{Jones72astatistical} is a weighting scheme broadly used in many tasks. Its goal is to reduce the impact of words that occur too frequently in a given corpus TF-IDF is the product of two statistic metrics, term frequency and inverse document frequency.
We calculate the TF-IDF scores for all pre-processed unigrams in the training corpus. The number of features equals the size of the vocabulary of the dataset and represents the dimensionality of the document vector.

\textbf{Information Gain} is used for feature selection \cite{Cover:2006:EIT:1146355}. In machine learning, Information Gain provides a method to calculate the mutual information between the features and the classification labels. According to \cite{Aggarwal12}, mutual information is defined on the basis of the level co-occurrence between the label and word. In other words, it represents the predictive power of each feature, and measures the number of bits of information obtained for prediction of a class in terms of presence or absence of a feature in a document. The Information Gain scores show how common is a specific feature in a target class, for example, those words that occur mainly in tweets labelled as FAVOR will be highly ranked. All the weights are normalized and the features ranked from one to zero. We then select those features that are larger than zero.

\textbf{Grid Search} is performed for hyper-parameter optimisation. The grid-search results are measured by 5-fold cross-validation on the training set. To reduce the cost of the grid-search process, we select two of the SVM (RBF kernel) parameters, namely, C and gamma.

\subsection{SVM+FastText Embeddings}\label{sec:svm+f-embedd}

Word embeddings encode words as continuous real-valued representations in a low dimensional space. Word embeddings are trained over large corpora and are able to capture semantic and syntactic similarities based on co-ocurrences. Word embeddings allow to build rich representations of text and have enabled improvements across most NLP tasks.

To the best of our knowledge, the only publicly available pre-trained models for both Catalan and Spanish are those distributed by FastText \cite{Grave18}. Initial experimentation showed that the Common Crawl\footnote{\url{http://commoncrawl.org/}} models performed better for our particular task. The Common Crawl models are trained using a Continuous Bag-of-Words (CBOW) architecture with position-weights and 300 dimensions on a vocabulary of 2M words. In order to produce vectors for out-of-vocabulary words, FastText word embeddings are trained with character n-grams of length 5, and a window of size 5 and 10 negatives \cite{Grave18}. We represent the tweet as the average of its word vectors \cite{DBLP:journals/corr/KenterBR16}, which is calculated as follows:

\[V(t)=\frac{1}{n}\sum_{i=1}^{n}W_{i}\]

where \textit{V(t)} is the vector representing a tweet, \textit{n} is the number of words and \textit{W} the vector for each word. In order to facilitate the look-up into the pre-trained word embedding model, the pre-processing described in the previous section is modified, leaving untouched the diacritics and the stopwords.

\subsection{FastText System}\label{sec:fasttext-system}

Apart from the pre-trained word embedding models, fastText also refers to a text classification system \cite{joulin-etal-2017-bag}. The fastText system consists of a linear model with rank constraint. A first weight matrix A is build via a look-up table over the words. Then the word representations are averaged to construct the tweet representation, which is then fed into a linear classifier. This is similar to the previous approach, but in the fastText system the textual representation of the tweet is a hidden variable which can be reused. The CBOW model proposed by \cite{mikolov2013b} is similar to this architecture, with the difference that the middle word is replaced by the stance label. Finally, fastText uses a softmax function to calculate the probability distribution over the predefined classes.

We use the fastText system in its default parameters, with the following exceptions: (i) instead of training the word embeddings online, we provide as input the pre-trained FastText word embedding models for Catalan and Spanish described in the previous section and, (ii) we use bag of bi-grams and trigrams as additional features with the aim of capturing word order information.

\subsection{Neural Architecture}\label{sec:neural-architecture}

Flair refers to both a deep learning system and to a specific type of character-based contextual word embeddings. FastText generates static word embeddings, that is, they provide a unique vector-based representation for a given word independently of the context, which means that polysemous words will always be given the same representation. Instead, contextual word embeddings are proposed to generate different word representations according to the context in which the word appears. Examples of such contextual representations are ELMo \cite{Peters:2018} and Flair \cite{akbik-etal-2018-contextual}, which are built upon LSTM-based architectures and trained as language models.

The Flair toolkit \cite{akbik-etal-2019-flair} allows to train sequence labelling and text classification models based on neural networks. Flair provides a common interface to use and combine different word embeddings, including both Flair and FastText embeddings. For text classification the computed word embeddings are fed into a BiLSTM to produce a document level embedding which is then used in a linear layer to make the class prediction. For best results, we follow their advice of combining in a stack their contextual Flair embeddings for Spanish with the FastText embeddings already mentioned \cite{akbik-etal-2018-contextual}. Every result reported with Flair is the average five training runs initialized at random.

\subsection{Evaluation}\label{sec:evaluation}

The models are tuned via cross-validation for the TW-10 dataset. The Catalonia Independence Corpus provides a development set which is used for tuning the models during training. The metric used by the organizers of SemEval 2016 \cite{mohammad-etal-2016-semeval} and IberEval 2018 \cite{taule18} reported the F1 macro-average score of two classes: FAVOR and AGAINST, although the NONE class is also represented in the test data. We use the provided evaluation script \footnote{\url{http://alt.qcri.org/semeval2016/task6/data/uploads/eval_semeval16_task6_v2.zip}} that calculates the final F1 macro score:

\[F1_{macro} = \frac{F1_{favor} + F1_{against}}{2}\]

\section{TW-1O Referendum Dataset}\label{sec:tw1o_dataset}

The TW-10 for IberEval 2018 dataset was collected by the hashtags \#1oct, \#1O, \#1oct2017 and \#1octl6 to obtain the tweets from Twitter\cite{taule18}. These hashtags were widely used in the debate on the right to hold a referendum on Catalan independence on the 1st of October 2017. A total of 87,449 tweets in Catalan and 132,699 tweets in Spanish were collected between the 20th and 30th of September. The final dataset consists of 11,398 tweets: 5,853 written in Catalan (the TW-1O-CA corpus) and 5,545 in Spanish (the TW-1O-ES corpus). The dataset was annotated manually by three experts. Also, each tweet is given together with its previous and next tweets as context. Table \ref{tab:length_tw_dataset} shows the average length of tweets after concatenating the tweet with its context.

\begin{table}[!ht]
\begin{tabular}{llc} \hline
  TW-10 corpus & Catalan & Spanish \\ \hline
  Average tweet length (tokens) & 37.69 & 38.86\\ \hline
\end{tabular}
\caption{Average length of tweets plus their context in the TW-1O corpus.}\label{tab:length_tw_dataset}
\end{table}


% Here is an example from TW-1O Referendum corpus:

% \begin{quote}
% TWEET: \textit{Res ni ning\'u, ens aturar\'a \#Votarem \#DretaDecidir \#1Oct \#CatalunyaLliure \#defensemlademocracia http://t.co/PgVLYH8AgN }

% LANGUAGE: Catalan

% STANCE: FAVOR

% Translation: \textit{Nothing and nobody will stop us \#Votarem \#DretaDecidir \#1Oct \#CatalunyaLliure \#defensemlademocracia http://t.co/PgVLYH8AgN}
% \end{quote}

Table \ref{tab:twdatasetdistr} illustrates the imbalanced nature of the Catalan subset, which makes it difficult to built and compare models for Catalan and across languages. Thus, while for Spanish the distribution of classes is quite similar, in Catalan the FAVOR class occurs 35 times more than AGAINST, and 8 times more than NONE.

\begin{table}[!ht]
\centering
\begin{tabular}{lcc} \hline
      Label & Catalan & Spanish\\ \hline
    Against & 120 & 1785 \\
      Favor & 4085 & 1680 \\
     None & 479 & 972 \\ \hline
      Total & 4684 & 4437 \\ \hline
\end{tabular}
\caption{Distribution of classes in the TW-1O trainset.}\label{tab:twdatasetdistr}
\end{table}

Tables \ref{tab:result_tw1o_ca} and \ref{tab:result_tw1o_es} reports our results for Catalan and Spanish respectively. It is clear that the Catalan subset makes it very difficult to perform any meaningful experiments given its class distribution. While the best approach for Catalan is SVM+TDF-IDF, it is clear that the results are heavily influenced by the under-represented AGAINST class.

\begin{table}[!ht]\small
\centering
\begin{tabular}{lccc} \hline
\textbf{System} & F1$_{against}$ & F1$_{favor}$ & F1$_{macro}$\\ \hline
SVM+TF-IDF & 22.86 & 94.68 & \textbf{58.77} \\
SVM+FTEmb & 0.00 & 93.88 & 46.94 \\
FastText+FTEmb & 12.90 & 94.60 & 53.78 \\
Flair+FTEmb & 14.79 & 94.40 & 54.59 \\ \hline
\textbf{Baseline} \\
\scriptsize{\cite{Cuquerella2018CriCaTM}} & - & - & 30.68 \\  \hline
\end{tabular}
\caption{Results on the TW-1O Catalan testset.}\label{tab:result_tw1o_ca}
\end{table}

\begin{table}[!ht]\small
\centering
\begin{tabular}{lccc} \hline
\textbf{System} & F1$_{against}$ & F1$_{favor}$ & F1$_{macro}$ \\ \hline
SVM+TF-IDF & 68.50 & 64.53 & 66.52 \\
SVM+FTEmb & 63.65 & 58.85 & 61.25 \\
FastText+FTEmb & 69.58 & 65.37 & \textbf{67.48} \\
Flair+FTEmb & 60.23 & 52.44 & 56.34 \\ \hline
\textbf{Baseline} \\
\cite{Segura-Bedmar18} & - & - & 28.02 \\ \hline
\end{tabular}
\caption{Results on the TW-10 Spanish testset.}\label{tab:result_tw1o_es}
\end{table}

The results for Spanish are a little bit more interesting. First, we can see that the fastText linear classifier combined with FastText embeddings (FastText+FTEmb) obtains much better results than SVM+FTEmb. As the document representation is the same, that means that the fastText classifier \cite{joulin-etal-2017-bag} improves over the performance of SVM. Finally, our results clearly improve over previous state-of-the-art in this dataset for both languages.

Still, and after seeing the results obtained for Catalan, we decided to propose a new multilingual corpus for stance detection with a better distribution of classes.

%\subsection{English Stance Dataset}
%The dataset in English was presented within the 2016 SemEval shared task \cite{mohammad-etal-2016-semeval} and consists of a collection of tweets with certain targets of stance: abortions, climate changes, Donald Trump and Hillary Clinton etc. Besides, we create new dataset in Catalan and Spanish labelled with stance towards broadly discussed topic - independence of Catalonia.

%To prepare the dataset, the organizers collected 2 million tweets containing favor, against and ambiguous hashtags for the selected targets. Each of the tweets was also annotated for whether the target of opinion expressed in the tweet is the same as the given target of interest. The organizers made a small list of query hashtags and split them into three categories: favor, against and ambiguous. Later, the hashtags were removed from the corpus. As only tweets with hashtags in the end of the tweet were used, the grammar and syntactic structure are kept. The authors organized a questionnaire and crowdsourcing setup for annotating stance. Each tweet was annotated by eight respondents \cite{Mohammad2016ADF}. The distribution of targets in the dataset is shown in the Table \ref{tab:eng_dataset}.

%\begin{table}[h]
%\centering
%\begin{tabularx}{\columnwidth}{|l|l|X|}
%\hline 
%Target              & Train & Test \\ \hline 
%Feminist Movement   & 664   & 285     \\ \hline
%Hillary Clinton     & 639   & 295   \\ \hline
%Legalization of Abortion    & 603   & 280   \\\hline
%Atheism             & 513   & 220   \\ \hline
%Climate Change is a Real Concern & 395  & 169   \\ \hline
%Total               & 2814  & 1249 \\ \hline

%\end{tabularx}
%\caption{Number of examples per target in the SemEval 2016 English dataset}
%\label{tab:eng_dataset}
%\end{table}


%Here is an example from the dataset. 


%TWEET: \textit{I still remember the days when I prayed God for strength.. then suddenly God gave me difficulties to make me strong. Thank you God! \#SemST}

%TARGET: Atheism

%STANCE: AGAINST
%%%

\section{Catalonia Independence Corpus 2019}\label{sec:independence}


During the process of developing the Catalonia Independence Corpus (CIC) we tried to address the main shortcomings of the TW-10 dataset, as shown by Section \ref{sec:tw1o_dataset}.

We had in our disposal a collection of tweets from 12 days in February and March of 2019 posted in Barcelona and from 30 days in September of 2018 posted in the town of Terrassa, Catalonia, prepared for commercial research in stance detection and political wing prediction. One of the characteristics of this collection is that it is crawled through API with full access to the data and the text of each tweet is not limited by 140 characters, so messages up to 240 characters are presented. We decided to take advantage of it and create a new dataset for academic research. First we separated them by language with Langdetect library\footnote{https://code.google.com/archive/p/language-detection/} and obtained 680000 tweets in Catalan and 2 million tweets in Spanish. We processed each set separately. We discarded tweets with identical messages and tweets containing less than three words of text after removing usernames.

\subsection{Annotating process}

For annotating, we used the following labels similar to  those from existing datasets.
\begin{itemize}
\item FAVOR: positive stance towards the independence of Catalonia.
\item AGAINST: negative stance towards the independence of Catalonia.
\item NEUTRAL: neither negative nor positive stance or impossible to make a conclusion.
\end{itemize}

\subsubsection{User level}

The annotation process is based on user classification, which  is one of the difference from TW-1O corpus that had been created with document annotation.

We started the annotation with making a list of Twitter accounts of media, political parties and political activists, whose opinion about Catalonia as an independent state is explicit. Also we extracted the most retweeted tweets from our corpus and categorized the authors of these tweets manually checking their Twitter accounts. We consider that for human, it is more easy to categorize the whole user account rather than the text of a single tweet without context. The conclusion about their opinion towards the topic was made not only by the messages but also  by  the special emojis used, such as yellow ribbon that symbolize the freedom of political prisoners, Spanish or Catalan flag, images or URLs provided  with the post, and by the self presentation in the Info section of the account. We assign stance FAVOR, AGAINST or NEUTRAL to each user.

We extracted relations between users using social network analysis based on retweets \cite{SNA2002}. Assuming that all those who make a retweet share the author's opinion, we categorized these users with the same label like the authors of the main message. With this method, in case that some person make a retweet without being agree with the topic, some noise can appear in the dataset.

In total, 25.510 users were categorized. We do not separate the users that write in Catalan and Spanish because most of the  active users in Catalonia are bilingual and may write in both languages, it depends on the context and situation. In the Table \ref{table:users} the distribution of categorized users is presented.

\begin{table}[!ht]
\centering
\begin{tabular}{lc} \hline
      Label & Count \\ \hline
      Favor & 22247 \\
      Against & 3091 \\
     Neutral & 176 \\ \hline
\end{tabular}
\caption{Distribution of the categorized users.}\label{table:users}

\end{table}

After assigning the labels to usernames, we obtain 131022 unique tweets in Catalan and 202645 unique tweets in Spanish.

\subsubsection{Topic level}

We annotated the corpus assigning the labels where a username is a label. However, although a user is categorized, in his account topics that are not related to politics may appear. In order to separate non-political tweets we apply the following filters.

\textbf{Hashtags and keywords}. We extracted all the hashtags from the corpus and selected manually all hashtags related to the independence of Catalonia, such as \textit{\#Catalu\~naesEspa\~na, \#CatalanRepublic, \#Tabarnia, \#GolpeDeEstado, \#independ\'encia, \#judicifarsa, \#CatalanReferendum} etc, 450 hashtags in total. Also we added keywords, such as \textit{independ\'encia, refer\'endum, separatisme} etc in both languages, 25 concepts in total. We marked each tweet if it contains ``independence'' hashtag or keyword.

The distribution of the tweets with the ``independence'' hashtags and keywords in the dataset is the following:

\begin{table}[!ht]
\centering
\begin{tabular}{lcc} \hline
Label & Catalan & Spanish\\ \hline
 Against & 1476 & 8267 \\
Favor & 23030 & 11843 \\
Neutral & 986 & 497 \\ \hline
\end{tabular}
\caption{Distribution of hashtags and keywords related to ``independence'' for the CID dataset.}
\end{table}

\textbf{Topic modelling}. As we see, there are much more tweets with the target topic labelled as FAVOR, than AGAINST and NEUTRAL. So, it is much more easy to obtain FAVOR class than others. In order to make the dataset balanced, we added more tweets to the minor classes. We applied latent Dirichlet allocation (LDA) algorithm \cite{Blei:2003:LDA:944919.944937} for topic modelling in order to separate non-political tweets from the AGAINST and NEUTRAL sets in Catalan collection and NEUTRAL set in Spanish collection. Here, the LDA topic modelling is some kind of a basic target detection algorithm.

LDA is widely used unsupervised machine learning technique which is able to detect topics in sets of unstructured text documents where each topic is characterized by a distribution over words. It is considered that LDA modelling over tweets performs fairly well though with some errors because of the documents length which is limited up to 240 characters. We use MALLET's LDA \cite{McCallumMALLET} topic modelling and topic coherence measure (U Mass) to estimate the best number of topics in the set of documents. We manually revised the topics and select only tweets which were clustered  as "independence" topic.

From each annotated collection we selected approximately 10.000 maintaining the proportion of the users in the initial dataset. We also keep only those tweets that contain more than four words. We did not do any manual revision after.

The average length of a tweet in Catalan Independence Dataset is slightly lower than in the TW-1O (see Table \ref{tab:length_tw_dataset}) but we should notice that in the TW-1O corpus there are three tweet concatenated and in CI only one, and also the shortest documents were filtered out.

In the Table \ref{tab:length_ind_dataset}

\begin{table}[!ht]
\centering
\begin{tabular}{lcc} \hline
CID Corpus & Catalan & Spanish \\ \hline
Average tweet length (tokens)& 27.17 & 30.31 \\ \hline
\end{tabular}
\caption{Average tweet length in the Catalonia Independence Dataset (CID) after removing usernames.}\label{tab:length_ind_dataset}
\end{table}

Here is an example of a tweet from Catalonia Independence dataset.

\begin{quote}
TWEET. \textit{Puigdemont visitar\`a el dia 13 de febrer la Universitat de Groningen dels Pa\"isos Baixos i presentar\'a  La crisi catalana, una oportunitat per Europa. \'Es un goig veure com ens reben els pa\"isos democr\`atics https://t.co/O38mDKwwn3}

STANCE: FAVOR

LANGUAGE: Catalan

Translation: \textit{Puigdemont will visit
on February, 13 the University of Groningen of the Netherlands and present The Catalan Crisis, An Opportunity For Europe.
It's a pleasure to see how democratic countries are receiving us https://t.co/O38mDKwwn3}
\end{quote}

In the Table \ref{tab:distr_dataset} distribution of classes in the dataset is given to illustrate that the FAVOR and AGAINST classes are perfectly balanced.


\begin{table}[!ht]
\centering
\begin{tabular}{lcc}\hline
Label & Catalan & Spanish \\ \hline
Against & 3988 & 4105 \\
Favor & 3902 & 4104 \\
Neutral & 2158 & 1868 \\ \hline
Total & 10048 & 10077 \\ \hline
\end{tabular}
\caption{Distribution of classes in the Catalonia Independence Dataset.}\label{tab:distr_dataset}
\end{table}

Developed dataset if divided in three parts: train, validation and  test in proportion 0.6, 0.2 and 0.2 respectively.

We applied proposed systems to the developed dataset and compare the performance in the Tables \ref{tab:result_indep_es}, \ref{tab:result_indep_ca}

\begin{table}[!ht]
\centering
\begin{tabular}{lccc}\hline
\textbf{System} & F1$_{against}$ & F1$_{favor}$ & F1$_{macro}$ \\ \hline
SVM+TF-IDF & 70.67 & 71.50 & 71.09 \\
SVM+FTEmb & 64.24 & 62.51 & 63.38 \\
FastText+FTEmb & 73.20 & 71.13 & \textbf{72.43} \\
Flair+FTEmb & 61.76 & 54.84 & 58.29 \\ \hline
\end{tabular}
\caption{Results on the Spanish testset of the Catalonia Independence Corpus (CIC-ES).}\label{tab:result_indep_es}
\end{table}

\begin{table}[!ht]
\begin{tabular}{lccc}\hline
\textbf{System} & F1$_{against}$ & F1$_{favor}$ & F1$_{macro}$ \\ \hline
SVM+TF-IDF & 68.89 & 72.91 & 70.90 \\
SVM+FTEmb & 59.43 & 64.46 & 61.95 \\
FastText+FTEmb & 70.73 & 72.21 & \textbf{71.47} \\
Flair+FTEmb & 59.08 & 58.08 & 58.96 \\ \hline
\end{tabular}
\caption{Results on the Catalan testset of the Catalonia Independence Corpus (CIC-CA).}\label{tab:result_indep_ca}
\end{table}


\section{Discussion}\label{sec:discussion}

With respect to Spanish, After revising the confusion matrix and some wrong predictions, we can conclude that the most common errors for the Spanish models are due to the true AGAINST class being predicted as FAVOR. In Catalan, AGAINST is often predicted as FAVOR or NEUTRAL because the algorithm is highly biased towards the majority class (FAVOR), and the minority class (AGAINST) is misclassified. As each tweet in this dataset was given in the context with the previous and the next tweet, the errors occur in the documents where code-switching or changing of topic is observed. Short documents with one or two tokens after pre-processing are likely to be misclassified, too.


In this section we summarize the results of proposed systems for automatic stance detection and compare the systems based on different corpus.

Conducted experiments show that the dataset built on user classification and topic modelling may be the base for automatic stance detection system. It allows to get a larger data from limited resources and make the process of annotation cheaper. It permits to regulate the class distribution within the corpus with higher flexibility adding or removing users or topics. Also, this method makes the corpus more homogeneous in respect of the topic of each tweet, as it is not mixed with the context tweets. The results shown in the Table \ref{tab:dataset_compare} prove that the systems trained on the dataset created with our method perform better than the same algorithms applied on the dataset annotated manually.

We should admit that our systems show similar behaviour on both datasets.

SVM models are not time and resource consuming and may be implemented on relatively noisy data. At the same time, the results obtained are still competitive with respect to newer deep learning systems. In this sense, it seems that neural architectures require larger training data to achieve better results.


\todo[inline]{We also say something about the different methods in which the datasets have been built, making a comparison about them and saying why the independence dataset is the result of an interesting methodology to build twitter-based datasets}

% \begin{table}[t]
% \begin{tabularx}{\textwidth}{|l|l|l|l|X|}
% \hline
% \textbf{System} & \textbf{TW-1O ES} & \textbf{TW-1O CA} & \textbf{CI ES}&\textbf{CI CA}\\
% \hline
% TF-IDF+SVM&0.6652&0.5877& 0.7109& 0.709\\
% \hline
% FastText+SVM&0.6125& 0.4694& 0.6338& 0.6195\\
% \hline
% Neural&           &          &       & \\
% \hline
% \end{tabularx}
% \caption{Results of proposed stance detection systems for each dataset. }
% \label{tab:dataset_compare}
% \end{table}


\section{Concluding Remarks}

In this paper we presented a new dataset for stance detection task: Catalan Indepencence 2019 - collected on Twitter data; and a method semi-automatic data annotation.

It should be underlined that in the Spanish and Catalan datasets, there is still a wide margin to test state of the art NLP approaches to build better stance classifications systems.

\section{Acknowledgements}\label{sec:acknoledgements}

This work has been funded by the~Spanish Ministry of Science, Innovation and Universities under the project DeepReading (RTI2018-096846-B-C21) (MCIU/AEI/FEDER, UE) and by the BBVA Big Data 2018 ``BigKnowledge for Text Mining (BigKnowledge)'' project. The second author is funded by the Ramon y Cajal Fellowship RYC-2017-23647. We also acknowledge the~support of the Nvidia Corporation with the~donation of a Titan V GPU used for this research.

% \nocite{*}
\section{Bibliographical References}\label{main:ref}

\bibliographystyle{lrec}
\bibliography{bibliography}


%\section{Language Resource References}
%\label{lr:ref}
%\bibliographystylelanguageresource{lrec}
%\bibliographylanguageresource{lrec2020W-xample}

\end{document}



